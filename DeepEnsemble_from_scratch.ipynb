{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0On7LtjAnf6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModel\n",
        "\n",
        "# --------------------------\n",
        "# Basic MLP for Toy Dataset\n",
        "# --------------------------\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, n=2, m=10, h=4):\n",
        "        \"\"\"\n",
        "        n: output_dim (number of classes)\n",
        "        m: input_dim\n",
        "        h: hidden_dim\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(m, h)\n",
        "        self.fc2 = nn.Linear(h, n)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))\n",
        "\n",
        "# --------------------------\n",
        "# BERT-based Classifier (Optional)\n",
        "# --------------------------\n",
        "class BERT(nn.Module):\n",
        "    def __init__(self, model_name, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "        self.classifier = nn.Linear(self.model.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output = self.model(input_ids, attention_mask)\n",
        "        pooled = output.last_hidden_state[:, 0, :]  # CLS token\n",
        "        return self.classifier(pooled)\n",
        "\n",
        "# --------------------------\n",
        "# Deep Ensemble Wrapper\n",
        "# --------------------------\n",
        "class DeepEnsemble(nn.Module):\n",
        "    def __init__(self, num_models):\n",
        "        super().__init__()\n",
        "        self.models = [MLP(n=2, m=10, h=4) for _ in range(num_models)]\n",
        "\n",
        "    def fit(self, train_data, val_data, epochs):\n",
        "        \"\"\"\n",
        "        Trains each model independently using adversarial training.\n",
        "        \"\"\"\n",
        "        for model in self.models:\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "            model.train()\n",
        "\n",
        "            for epoch in range(epochs):\n",
        "                for x, y in train_data:\n",
        "                    x.requires_grad_()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    # Standard loss\n",
        "                    output = model(x)\n",
        "                    loss = criterion(output, y)\n",
        "                    loss.backward()\n",
        "\n",
        "                    # Adversarial loss (FGSM-style)\n",
        "                    x_adv = x + 0.001 * x.grad.sign()\n",
        "                    optimizer.zero_grad()\n",
        "                    output_adv = model(x_adv)\n",
        "                    loss_adv = criterion(output_adv, y)\n",
        "\n",
        "                    # Total loss\n",
        "                    total_loss = loss + loss_adv\n",
        "                    total_loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                # Optional: validation logging\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    val_loss = []\n",
        "                    for x, y in val_data:\n",
        "                        output = model(x)\n",
        "                        loss = criterion(output, y)\n",
        "                        val_loss.append(loss.item())\n",
        "                    print(f\"Val Loss: {sum(val_loss)/len(val_loss):.4f}\")\n",
        "\n",
        "    def inference(self, x):\n",
        "        \"\"\"\n",
        "        Collects predictions from all models and computes:\n",
        "        - mean prediction\n",
        "        - variance (epistemic uncertainty)\n",
        "        - aleatoric uncertainty estimate\n",
        "        \"\"\"\n",
        "        predictions = []\n",
        "        for model in self.models:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                pred = torch.softmax(model(x), dim=-1)\n",
        "                predictions.append(pred)\n",
        "\n",
        "        stack = torch.stack(predictions, dim=0)       # [num_models, batch, num_classes]\n",
        "        mean = torch.mean(stack, dim=0)               # Ensemble mean\n",
        "        var = torch.var(stack, dim=0)                 # Epistemic uncertainty\n",
        "        aleatoric = torch.mean(stack * (1 - stack), dim=0)  # Aleatoric uncertainty\n",
        "\n",
        "        return predictions, mean, var\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# === Generate dummy classification data ===\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2)\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "X_train = torch.tensor(x_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test = torch.tensor(x_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=64)\n",
        "\n",
        "# === Train the ensemble ===\n",
        "ensemble = DeepEnsemble(num_models=5)\n",
        "ensemble.fit(train_loader, val_loader, epochs=3)\n",
        "\n",
        "# === Inference on single example ===\n",
        "x = torch.rand(1, 10)\n",
        "_, mean, var = ensemble.inference(x)\n",
        "\n",
        "print(\"Mean prediction:\", mean)\n",
        "print(\"Uncertainty (variance):\", var)\n"
      ],
      "metadata": {
        "id": "9c10v9qABJeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModel\n",
        "\n",
        "# --- MLP for Regression with Uncertainty ---\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, n=1, m=10, h=4):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(m, h)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc_mean = nn.Linear(h, n)\n",
        "        self.fc_var = nn.Linear(h, n)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.relu(self.fc1(x))\n",
        "        mu = self.fc_mean(h)\n",
        "        var = F.softplus(self.fc_var(h)) + 1e-6  # Ensure positive variance\n",
        "        return mu, var\n",
        "\n",
        "# --- Optional BERT Wrapper (Unused Here) ---\n",
        "class BERT(nn.Module):\n",
        "    def __init__(self, model_name, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "        self.classifier = nn.Linear(self.model.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output = self.model(input_ids, attention_mask)\n",
        "        pooled = output.last_hidden_state[:, 0, :]\n",
        "        return self.classifier(pooled)\n",
        "\n",
        "# --- Deep Ensemble for Regression with Uncertainty ---\n",
        "class DeepEnsemble(nn.Module):\n",
        "    def __init__(self, num_models):\n",
        "        super().__init__()\n",
        "        self.models = [MLP(n=1, m=10, h=4) for _ in range(num_models)]\n",
        "\n",
        "    @staticmethod\n",
        "    def GaussianNLLLoss(mu, y, var):\n",
        "        return torch.log(var) / 2 + ((y - mu) ** 2) / (2 * var)\n",
        "\n",
        "    def fit(self, train_data, val_data, epochs):\n",
        "        for model_idx, model in enumerate(self.models):\n",
        "            criterion = nn.GaussianNLLLoss(reduction='mean')\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "            model.train()\n",
        "\n",
        "            for epoch in range(epochs):\n",
        "                for x, y in train_data:\n",
        "                    optimizer.zero_grad()\n",
        "                    mu, var = model(x)\n",
        "                    loss = criterion(mu.squeeze(), y.squeeze(), var.squeeze())\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                # Optional: val loss logging\n",
        "                model.eval()\n",
        "                val_loss = []\n",
        "                with torch.no_grad():\n",
        "                    for x, y in val_data:\n",
        "                        mu, var = model(x)\n",
        "                        loss = criterion(mu.squeeze(), y.squeeze(), var.squeeze())\n",
        "                        val_loss.append(loss.item())\n",
        "                print(f\"Model {model_idx+1} | Epoch {epoch+1} | Val Loss: {sum(val_loss)/len(val_loss):.4f}\")\n",
        "\n",
        "    def inference(self, x):\n",
        "        mus, vars_ = [], []\n",
        "        for model in self.models:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                mu, var = model(x)\n",
        "                mus.append(mu)\n",
        "                vars_.append(var)\n",
        "\n",
        "        mu_stack = torch.stack(mus)          # [num_models, batch, 1]\n",
        "        var_stack = torch.stack(vars_)       # [num_models, batch, 1]\n",
        "\n",
        "        mean_pred = mu_stack.mean(dim=0)\n",
        "        epistemic = mu_stack.var(dim=0)\n",
        "        aleatoric = var_stack.mean(dim=0)\n",
        "\n",
        "        return mean_pred, epistemic, aleatoric\n"
      ],
      "metadata": {
        "id": "he6zrSkNB5w4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble = DeepEnsemble(num_models=5)\n",
        "ensemble.fit(train_loader, val_loader, epochs=3)\n",
        "mean, epistemic, aleatoric = ensemble.inference(torch.rand(1, 10))"
      ],
      "metadata": {
        "id": "STc45fWLCLBN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}